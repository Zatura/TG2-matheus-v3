\documentclass[ecp,tc,english]{iiufrgs}
% 
% Outras Opções:
% * english    -- para textos em inglês
% * openright  -- Força início de capítulos em páginas ímpares (padrão da
% biblioteca)
% * oneside    -- Desliga frente-e-verso
% * nominatalocal -- Lê os dados da nominata do arquivo nominatalocal.def


\usepackage[utf8]{inputenc}   % pacote de acentuação unicode
\usepackage{graphicx}         % pacote de importar figuras
\usepackage{times}            % pacote de fonte Adobe Times
% \usepackage{palatino}
% \usepackage{mathptmx}       % pacote de fonte Adobe Times nas fórmulas
\usepackage{fixmath}          % pacote de math international standardization
\usepackage[alf,abnt-emphasize=bf]{abntex2cite}	% pacote de citações abnt

\newtheorem{definition}{Definition}

\title{User sessions content-based identification, considering multiple accounts}

\author{Tura}{Matheus Toazza}
\advisor[Prof.~Dr.]{Cordeiro}{Weverton Luis da Costa}
\coadvisor[Prof.~Dr.]{Galante}{Renata de Matos}
\location{Porto Alegre}{RS}
\date{May}{2021}

\keyword{Dimensionality reduction}
\keyword{clustering}
\keyword{user profiling}
\keyword{session identification}
\keyword{recommendation systems}

%\settowidth{\seclen}{1.10~}

\begin{document}

\maketitle

% dedicatoria
% \clearpage
% \begin{flushright}
%     \mbox{}\vfill
%     {\sffamily\itshape
%       ``If I have seen farther than others,\\
%       it is because I stood on the shoulders of giants.''\\}
%     --- \textsc{Sir~Isaac Newton}
% \end{flushright}

% agradecimentos
%\chapter*{Agradecimentos}
%Agradeço ao \LaTeX\ por não ter vírus de macro\ldots


\begin{abstract}
  Although some content providers register stream data from its users and can track their profile style for content recommendation, when two or more users share a same account, their true profile activity is obfuscated and fuzzed, this user behavior hinders the recommendation systems from providers. This work proposes a way of classifying users' stream data trough sessions based on its media content, opening the possibility for breaking a same account profile within multiple user profiles and consequently identifying this activity. In this work dimensionality reduction and clustering methods are used to classify user stream data into sessions that correspond to each respective user profile.
\end{abstract}

% resumo em português
\begin{englishabstract}{}{Redução de dimensionalidade, Clusterização, Perfis de usuários, Identificação de sessões, Sistemas de recomendações}

  Embora as provedoras de conteúdos registram dados de acessos de seus usuários e consigam analisar seus perfis para recomendações de conteúdo, quando duas ou mais pessoas compartilham da mesma conta a atividade e perfil original e individual de cada usuário é obfuscada e difusa por essas contas compartilhadas, este comportamento confunde os sistemas de recomendação existentes. 
  Este trabalho propõe uma maneira de classificar o fluxo de dados dos usuários em sessões baseando-se em seu conteúdo, abrindo portas para quebrar a mesma conta em múltiplos perfis de usuários e consequentemente identificando esta atividade. Neste trabalho Redução de dimensionalidade e métodos de clusterização são utilizados para classificar o fluxo de dados em sessões que correspondem respectivamente a cada perfil de usuário.
\end{englishabstract}

\listoffigures 
\listoftables 

\begin{listofabbrv}{SPMD}
    \item[PCA] Principal Component Analysis
    \item[tSNE] t-Distributed Stochastic Embedding
    \item[ALS] Alternate Least Squares
    \item[RNN] Recurrent Neural Networks
\end{listofabbrv}

\tableofcontents % sumario

% =======================================================================

%TODO No final da introdução fazer um paragrafo explicando brevemente cada capítulo.-7  
\chapter{Introduction}
Most platforms nowadays rely on recommendation systems to create plug and play interactions between users and offered items, avoiding search time and making platform navigation minimal and crisp, however, some user behaviours or even the emerging privacy laws and growing concerns on data privacy, recommendation systems precision can get affected by profiling contamination on the user-item relations  and its interpretation by the system.

One commonly known example responsible for grinding the recommendation systems performance is the practice of account sharing that mixes and masks its account profiles against its real users. Account sharing behaviour is non-intentional in most cases, since device sharing is common, there could be a situation where some individual into his computer is searching videos in a logged-in streaming platform for specific subjects like for example C++ usage, but at some point, it leaves the computer empty without performing account logout, another individual then starts using the same device and platform but for a completely different subject, like children's cartoon.

This problem is evident on commonly used time-based session identification techniques such as \cite{10.1145/2736277.2741117}, the presented scenario would fail in identifying sessions, if chosen period threshold for breaking sessions is long enough, the elapsed time from user permutation usage on platform could be insignificant on the total session time, thus preserving the unity of the session, but in reality two completely sessions have happened during this period.

On \cite{Jiang:2018:IUB:3209978.3210054} its main goal is identifying shared accounts in multimedia streaming and it addresses a clearly domain separation between \textit{account-level recommendation} and \textit{user-level recommendation}, but even so, they use a timeout along with their other techniques to break the session.


% Por que é importante identificar com a máxima precisão possível essas sessões? 

% Aqui, a resposta está nos trabalhos que usam clusterização de sessões para identificar perfis de usuários que navegam pelas plataformas usando contas compartilhadas. Aqui vale citar o trabalho de Jiang et al. (2017). Aqui vale destacar que esse trabalho, além de vários outros (incluir citações) usa timeout pra quebrar as sessões.

% Vale discutir quais os problemas de usar timeout pra esse fim. 
% General objective: 
% O objetivo geral é verificar a viabilidade de detecção automática de sessões de usuários em plataformas online (como por exemplo de e-commerce) usando outras técnicas além de timeout. 
% Specific Objectvies
% Definir uma metodologia para a avaliação da eficácia e eficiência de diferentes técnicas na identificação de sessões. Outro objetivo específico é avaliar a viabilidade do uso da técnica X e da técnica Y, aplicando-se para esse fim a metodologia desenvolvida no escopo do trabalho. Justificar a escolha das técnicas.

% Após detalhado um exemplo para contextualizar o trabalho, vem os objetivos geral e específicos. O objetivo geral é verificar a viabilidade de detecção automática de sessões de usuários em plataformas online (como por exemplo de e-commerce) usando outras técnicas além de timeout. 

% Um objetivo específico é definir uma metodologia para a avaliação da eficácia e eficiência de diferentes técnicas na identificação de sessões. Outro objetivo específico é avaliar a viabilidade do uso da técnica X e da técnica Y, aplicando-se para esse fim a metodologia desenvolvida no escopo do trabalho. Justificar a escolha das técnicas.

% Para a avaliação desse trabalho, foram utilizados traços tal e tal, que tem tais características (em resumo; mais adiante no documento, tu vai apresentar uma visão mais detalhada das características dos traços usados). 


% O restante do trabalho está organizado como segue. No Capítulo 2 são apresentados os principais trabalhos relacionados. No Cap. 3 .... . No Cap. 4 ... . 


The proposed work consists in two global objectives: (i) Verifying the possibility of an algorithm for session identification based on item category instead of timeout, 
(ii) Create an evaluation method for reliably testing its accuracy and precision.

globo.com dataset offered a good amount of data, it was preferred instead of others, but since they do not allow the use of news content, the [article] made available its embeddings that carries 250 dimensions of features from more than 300,000 articles. A dimensionality reduction technique was needed in order to reduce complexity of data and apply the \(Affinity Propagation\) clustering algorithm,

The image below describes in general terms the pipeline of both proposals

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\textwidth]{images/experiment.png}
    \caption{Proposed validation method}
    \label{fig:method_architecture}
\end{figure}

The dataset of choice in order to test its techniques was  \textit{globo.com dataset} from \cite{deSouzaPereiraMoreira:2018:CDL:3240323.3240331} 
that aggregates streams of user clicks with more than 2.8 millions of clicks.

%TODO Deve ser atualizado
On chapter 3 the concepts, history and motivation of choice on used algorithms are presented. Chapter 4 presents the problem and the solution definition.
At the Fifth chapter, experimentation is revealed as its used tools, metrics are applied in order to measure accuracy and reliability of the proposed techniques.

% =======================================================================

\chapter{Theoretical Background and Related Work}
Here you will find the foundations, they will be introduced as well as related works, those are crucial to assemble the complete picture of this work.

At subsection \ref{basic_concepts}, basic concepts and attributes are introduced and on subsection \ref{basic_methods} methods and algorithms are introduced with its applicability and past usage. Related works are presented on subsection \ref{related_works}

    \section{Basic concepts} \label{basic_concepts}
        % TODO explain each item
        
        \begin{itemize}
            \item \(User\) is the person itself that is using the platform trough an account, most of times each user has its own account, but at sometimes, one account can have multiple users.        
            \item \(Account\) is a platform gateway and record that contains user detailed information and is used to register user history associating each requested item with its requester account id.
            \item \(Item\) represents the consumed media, can be music, image, video or text in case of a news article.
            \item \(Clickstream\) is a group of requested items ordered by request timestamp.    
            \item \(Session\) is a subset of a stream, the beginning and end of a session can be marked considering user idleness, where the time difference between one requested item and the next one is higher than a certain threshold.
        \end{itemize}

        % User and Account
        An account is the common gateway for an user to access the platform content, most of past works consider that each account is in fact one user which may happen on most scenarios because normally each user has its own account, but sometimes and even more on paid platforms, an account is shared between more users.
    
        % Item and Session        
        An item is a pair with a click timestamp and a media id identification. 
        At Globo dataset, each item is a  set \((timestamp, click\_id)\) but 
        in any other dataset, the click id is a reference to news, music, 
        image, video or any metadata associated. A session is a well defined 
        segment from a stream of items from one account. There are many ways 
        of defining session boundaries, on \cite{10.1145/362883.362920} the session 
        is defined by using time threshold, same as \cite{10.1145/2736277.2741117}. 
        For example when a user stays inactive for an amount of time, that can be 
        considered an end of a session and start of another. In this proposal the 
        sessions will be split considering the content type in order to identify 
        account sharing or even user profiling for different user tastes.


    \section{Basic methods} \label{basic_methods}
        \subsection{Recommendation system}
        The role of a \textit{recommendation system} is to make inferences of new \(items\) that the \(user\) would like and suggest them. It is a set of algorithms that together they can return for example an ordered list of probability/similarity from items that were previously compared against other items. These other items must be similar from the ones that the \(account\) have interacted or similar accounts have interacted with.
        Variables such as click-rate, visit frequency, time spent on page can be considered to calculate an affinity between interacted \(items\) and \(account\).

        A recommendation system can have four main classifications:
        \begin{itemize} 
            \item Implicit vs Explicit
            \item Content-based vs Collaborative
        \end{itemize}

        \textit{Implicit vs Explicit} definitions correspond to the way data is semantically related between the pair \(account\)-\(item\). On \(Implicit\) recommendation systems there is an implicit relationship between the used item and the expectation that the majority of those items are desirable by its user, on the other side of the spectrum, there are the \(Explicit\) recommendation systems that usually uses a rate system given by the \(user\), so there is not just a simple link between both entities, but also a subjective magnitude that could be either positive or negative, qualitatively speaking.

        \textit{Content-based vs Collaborative} classifies on an \(account\)-\(account\) relationship. On the first one the similarity between interacted items on a same account is used to retrieve similar new items that the user didn't yet explored, on the second one, the comparison is based upon other accounts that share similarity, so items from these accounts that are missing on the interaction history can be recommended. On state of art systems, both methods are combined in order to get the best results.    
        
        % TODO subseção com apenas 1 parágrafo
        \subsection{Dimensionality reduction}
        The more dimensions a dataset has the more diluted the statistical significance becomes, 
        because data sparsity grows exponentially as the dimensions grow linearly,  
        this phenomena is called \textit{the curse of dimensionality} and is described on \cite{10.1093/imamat/24.1.59} and one on way to finding statistical significance are techniques of \textit{dimensionality reduction}, further discussions and solutions are discussed on \ref{data_scarcity_and_sparcity}.
        
        One of firsts successful algorithms is called \textit{Principal Component Analysis}, 
        its model was developed in 1933 by \cite{hotelling:33} and it consists in finding linear combinations between pair of dimensions that corresponds to most statistical significance trough variance, where the two most significant linear combinations becomes the main dimensions and its points are plotted in a two dimensional Cartesian plane.
        Dimensionality reduction techniques are used before applying Clustering algorithms, because normally media contents are described as an array of multidimensional features.
        
        % TODO subseção com apenas 1 parágrafo
        \subsection{Cluster analysis}
        It is the method for grouping items that share a vicinity/similarity and then labelling each one of the groups, normally, clustering algorithms are applied on points plotted on two-dimensional planes. There are many types of algorithms for classifying clusters, such as \textit{distribution-based}, \textit{density-based}, \textit{centroid-based}, \textit{connectivity-based} and others. In this work the centroid-based algorithm \(Affinity\ propagation\) was used for classifying clusters of similar \(items\), in this case each \(item\) represented by one news content.

% =======================================================================
% More related works
\section{Related works} \label{related_works}
% This chapter will present previous works that used user identification and sessionization as solution, both profile-level and time-level methodologies.
    
\cite{DBLP:journals/corr/ZhangFIM14} addressed the problem of account sharing and modelled an algorithm for identifying movie ratings from individual users sharing an account. It is used a model of union of linear subspaces for spotting shared accounts and a model of clustering for user identification per account. This algorithm has good accuracy on most cases, showing that it is possible to identify a distinction from each user based on item category and action, however, this algorithm is restricted for explicit recommendation systems and movie datasets tend to be smaller in size and category amount in comparison to other media datasets.

The work \cite{10.1145/2736277.2741117} consists in monitoring clickstream of different domains on online activity browsing (Games, search engines, page views), thus achieving session identification using a fixed 30 minutes inactivity threshold as session identification, authors demonstrates high regularity between different sessions from a same user. 
Since they do not address account sharing, there is a chance that threshold based on item usage or user behaviour instead of time usage, would have more impact on identifying sessions related to different users.

On \cite{JINDAL2020} et al, they propose a dynamic threshold heuristic based on user behaviour history, their discoveries show that short time user navigation are more often and less correlated than longer ones, making them easier to categorize and achieving a higher accuracy. However, this method apply a threshold into the user behaviour pattern, and differently, this work relies on the distance between topics inside a session, also the proposed method uses fixed threshold for simplicity reasons.

\cite{10.1145/2792838.2800170} proposes a top-N recommender system for solving account sharing problems, three common problems are categorized and solved:
\begin{itemize}
    \item \textbf{Generality problem} is when the recommended item is the mean of all user items, being uninteresting for none of the users.
    \item \textbf{Dominance problem}, happens when few users use the system more than the others, in that case most of items are related to those few users.
    \item \textbf{Presentation problem} addresses the difficulty of recommending an item for the current user and not the other idle users.
\end{itemize}

This model uses binary positive-only feedback as items and besides they used movie ratings datasets, their model probably can be used on implicit recommendation systems because they share the binary feedback model between user and items.
Their approach uses normalized cosine similarity with KNN \cite{oro28327} to produce the account-item score recommendation matrix, then they introduce their novel DAMIB-COVER algorithm that solves the generality problem by differentiating scores between a big sum of few similarities and a small sum of a lot similarities from users in a same account.

One of the state of art contribution is from \cite{Jiang:2018:IUB:3209978.3210054} that using the \textit{LastFM} and \textit{KKBOX} datasets, they address a solution on user identification on shared accounts at streaming platforms given its history logs, proposing the novel unsupervised learning named SHE-UI. They could be able to identify groups of users sharing a same account, bringing their solution to an user-level recommendation domain instead of account-level.
A problem that e-commerce recommendation systems face is when an advertisement of an exact size or individual characteristic for an user is required, because other users may end up contaminating the account history and spoiling the estimate of these unique attributes or sizes, on \cite{10.1145/3178876.3186149} et al the problem is discussed using Amazon.com e-commerce, instead of dealing with each account as a specific user and single profile, they propose a multi-persona approach boosting the accuracy, however, this method does not work well on products that have a high variability on its characteristics.




% Aqui tá faltando trabalho relacionado.
% =======================================================================

\chapter{Methodology}
At the current chapter, problem definition, algorithms, tools used for the proposed methods and its validations are discussed and presented with its motivations.

    \section{Problem definition}
    \begin{definition}
    Let \(I \) be a set of items where \(i \in I \) and an item \(i\) represents a news. Let a set of users \(U\) where \(u \in U \) and each user \(u\) does not repeat itself into the set.
    A session \(s\) is defined by a list of items \(s_{1} = \{i_{1}, i_{2}, i_{3}\}\) and belongs to an individual user \(u_{1}\), the items may repeat inside a session and also between sessions.
    \end{definition}
    
    It is considered by definition, that each User is an account itself, and that most of users doesn't share its account, and the amount of shared accounts have low  statistical significance into the end result.
    
        \subsection{Simulated Accounts and Sessions}
        In order to simulate a shared account, two sessions are concatenated into a new session as \(s_{1} = \{i_{1}, i_{2}, i_{3}\}\) and  \(s_{2} = \{i_{7}, i_{8}, i_{9} ...\}\) and those sessions must belong to different users.
    
        the new simulated session consists in the following representation
    
        \(s_{12} =  s_{1} \cup  s_{2} =  \{i_{1}, i_{2}, i_{3}, i_{7}, i_{8}, i_{9}, ...\}\)
    
        The objective of the proposed method is to identify the exact frontier-item between sessions and revert this concatenation process separating the new session  \(s_{12}\) into the original \(s_{1}\) and \(s_{2}\) sessions, in that scenario, item \(i_{3}\) would be the frontier-item, since it is the last item of the first session \(s_{1}\).
    
        This process should use only the item content or category as parameters, being blind to the user ids or timestamps.    
        % Organizar os trabalhos relacionados entre:
        
        % Secão 2.1: Investigações que dependem da identificação de sessões online de usuários. Aqui entra, por ex., o trabalho de Jiang et al. (2017). 
        
        % Secão 2.2: Investigações que discutem possíveis técnicas para quebrar sessões online dos usuários.
        
        Time-based sessions are the most commonly used as consensus as a method for example at \cite{10.1145/3184558.3191582} it is used a defined session trough a fixed time, but the way they make it flexible, were by using a decaying time-based weight into the session items, they used this approach to overcome the short life-span of news. At the proposed method, since not the news itself, but it's topic, the short life-span problem wasn't expected to be relevant enough.
    
        \subsection{Data scarcity and sparcity} \label{data_scarcity_and_sparcity}
        \textbf{Cold start problem.} This flaw is addressed mostly to \textit{content-based} recommendation systems, since it is content-centric, when an user is fresh and he has little or no content history, the method lacks on precision resulting on spurious extrapolations.
    
        A \textit{collaborative-based} system is no different in terms of this flaw, but it is more resistant since it collaboratively uses content history from other users, it can fill the gap against data scarcity from fresh users.
        
        % TODO refazer o primeiro método com 200 items também.
        In order to avoid this dilemma, the proposed metrics are based on previously filtered sessions, where the first method use sessions with more than 10 items, and the second method, more than 200 items.
    
        % Matheus: Necessário? na introdução este problema é brevemente descrito e seu autor citado é o motivo do uso de um algoritmo como o tSNE.
        \textbf{Curse of dimensionality Problem.} Since content analysis and classification requires a complex analysis that generates great amounts of features/dimensions and usually recommendation systems data are sparse, the curse of dimensionality \cite{10.1093/imamat/24.1.59} is very relevant as a problem because usually any media content should be translated into a multi dimensional space before the classification, contributing to a crescendo \textit{curse of dimensionality}.

        
    \section{Algorithms} \label{algorithms}
    
    On this section, all the datasets and used algorithms will be described as well as it historic evolution against their pioneers.
        
    \textbf{Principal Component Analysis} \cite{hotelling:33}  is the first known form of dimensionality reduction for multidimensional data. The pinnacle of the technique is mapping from all dimensions two linear combinations which have the biggest statistical significance, or in other terms, mapping on two dimensions which have maximized variance between all others.
    
    If you have \(n\) dimensions and \(m\) items where each item has its own coordinate, PCA would translate this \(m \times n\) space matrix into another rotated \(m \times n\) matrix trough linear operations into each dimension, those new translated dimensions are called \textit{principal components}.
    Their rotation is in fact a linear combination between two original dimensions which have its variance maximized using \textit{Least Squares Regression}.
    
    The \textit{principal components} end up ordered based on their variance from its span, they are the eigen vectors positioned on the diagonal of the resulting matrix and the rank of \textit{principal components} based on its variance is called \textit{scree-plot}.

    However, \textit{Principal component analysis} is the naive approach for data dimensionality reduction, but it is very useful when data have information linked trough linear relationships.
    A successful and recent example of PCA use is this physical engine in \cite{10.1145/3309486.3340245}, that before a complex process of vector computation for physics simulation, a PCA was used to reduce the statistical irrelevance creating a meaningful and denser subspace in terms of significant information before training a neural network, making the algorithm 10X faster and using less memory.
    
    At Globo dataset, as \cite{deSouzaPereiraMoreira:2018:CDL:3240323.3240331} mentions, they could not provide the news content due to its closed source policy, however, they released a matrix of features where each news was represented by an array of 255 features, also they mention in their article that tSNE would translate to a better representation of the reduced data, since this matrix have non-euclidean information relationships, so in that case, PCA would not fit the best 2-dimensional surface in terms of statistical significance.
    % Estou pensando em remover esta explicação abaixo
    To think what would happen using a excessively linear algorithm like PCA to find the best surface, imagine a zig-zagged bent bread and you want to cut the middle of it with a knife, PCA would make a straight line cut ignoring the format of the bread. In that case, the algorithm they recommend is tSNE, a modern and state-of-art technique for dimensional reduction that uses machine learning and gradient descend techniques.
    
    \textbf{tSNE (t-distributed stochastic neighbor embedding)} also maps highly dimensional data into a low dimensional representation resuming the data and resisting against dimensionality curse as mentioned \cite{10.1093/imamat/24.1.59}, but differently from PCA, tSNE \cite{vanDerMaaten2008} is a non-linear method, this method preserves local geometry and deform global geometry, more specifically, it tends to clump points that share a near neighborhood making the clusters more evident.
    Non-linear means that if you have highly dimensional data that has a geometric representation of a curvy non-convex surface of points, for example, PCA due to its linearity would make a straight cut into this curve finding straight planes and never really fitting into the curvy surface, PCA has its algorithms variances that reduce its problems, like the Kernel PCA, but tSNE is considered to be the state-of-art for reasons below.

    tSNE It is a sequel of SNE (Stochastic Neighborhood embedding), but because the current distribution from SNE not only hardly clump the points close to each other but it is use to be computationally slow, because of the use of exponentiation when calculating the distribution equation.
    % TODO show t-student formula, compare the performance against gaussian model
    The "t" letter stands for t-student distribution, it is computationally faster and has a less severe penalty on point agglutination due to its natural flattened curve, thus winning the "state-of-art" title due to its low cost and better precision.

    On figure \ref{fig:tsne_plot} there is a plot of tSNE applied on Globo dataset highly dimensional embedding that represents the content of each news, this exactly plot and its data was used on the proposed method, each point represented below is a relationship of a Xs, Ys coordinate with an article id 

    \begin{figure}[H]
        \centering
        \includegraphics[width=1\textwidth]{images/tsne_clusters.png}
        \caption{tSNE plot from globo dataset}
        \label{fig:tsne_plot}
    \end{figure}

    \textbf{Affinity propagation.} The advantage of affinity propagation between other algorithms is that it does not need to specify an estimate of quantity of points found into the space, it naturally walks trough the points and finds the resulting clusters.

    On figure \ref{fig:affinity_twins} there is the visualization of the discovered clusters on the low dimensional embedding representation from tSNE result of globo dataset, the second image shows the center from each topic cluster, that later should be used to calculate the semantic difference from each topic based on the euclidean distance from the pair of topics being compared.
     
    \begin{figure}[H]
        \centering
        \includegraphics[width=1\textwidth]{images/affinity_twins.png}
        \caption{Affinity propagation clusters(left) and its centers(right)}
        \label{fig:affinity_twins}
    \end{figure}
    
    % TODO \cite{10.5555/1087620} <- phd thesis do matrix factorization
    % \subsection{}
    \textbf{Matrix Factorization.} Netflix is known for it's competitions for collaborative filters algorithms, in 2008 \textit{matrix factorization} \cite{10.5555/1087620} was used and achieved a huge breakthrough in collaborative filtering algorithms, and it is like the prime factors for grown ups, using matrices as factors instead of just scalar numbers, it is a useful mechanism to separate the users-items matrix into a items' matrix and a users' matrix, that their multiplication result in the original users-items matrix, results into Item X User original association matrix that each item is the relation of one user to a single item.
    The usefulness of this factorization, is that since the original matrix is sparse, you can fill the missing values by making inferences on the factorized data, because not every user in its history has real relationship with every item.
    An optimal way of doing so is by using the \textit{Alternate Least Squares} algorithm explained below that uses  \textit{Gradient Descent} as convergence algorithm.
    
    
    \textbf{Alternate Least Squares.} It is a faster variant of the Matrix Factorization that allows it to be computational distributed, the as matrix factorization, it has SVD as central role on the factorization between item and user matrices, but in this case, a cost function with an error function is alternated between each matrix, updating the elements of one matrix at a time, this computational behaviour reduces the complexity of the cost function and allows it to be computed in a distributed fashion.
    Today a common implementation of ALS is from Spark \(Hadoop\) in java, but also have facade libraries that allow calling ALS from spark \(Hadoop\) from other languages like Scala and Python (Pyspark).

    \section{Technology usage}

    It was used \(Python3\) with \textit{pandas} \cite{reback2020pandas} and \textit{numpy} as main data manipulation, \(Pyspark\) was used for ALS method recommender system. 
    \(Pyspark\) is a python library that is a facade for a java spark hadoop spark application server that uses map/reduce in order to make distributed computations.

    For tSNE and Affinity propagation \(sklearn\) library was the choice.
        
        \subsection{Applying Dimensionality reduction}
        Because Globo news text data is closed source, \cite{deSouzaPereiraMoreira:2018:CDL:3240323.3240331} made available the trained article embeddings data \footnote{https://www.kaggle.com/gspmoreira/news-portal-user-interactions-by-globocom} generated from its unique \textit{CHAMALEON} RNN-based algorithm. The embedding file consists into a matrix of 250 dimensions of features and 364,047 news articles, those features are a representation of the content for each respective news article.
    
        On the proposed method only the following data and respective embedding were considered: \textit{click\_timestamp, article\_id} and \textit{{}user\_id}.
        
    
        Since on their work tSNE was mentioned as a good method for dimensionality reduction, tSNE was applied on the embedding, resulting into a pair of coordinates X, Y for each article id as the dataframe sample below:
        
        \begin{table}[!ht]
            \centering
            \begin{tabular}{ |c|c|c| } 
                \hline
                article\_id & x & y \\
                \hline 
                ... & ... & ... \\
                69 & 10.950659 & -26.211418 \\ 
                81 & 34.414822 & -0.690890 \\ 
                84 & 35.335995 & -1.578238 \\ 
                ... & ... & ... \\
                \hline
            \end{tabular}
            \caption{tSNE coordinate results}
            \label{tab:tsne_results}
        \end{table}
        
        \subsection{Applying Clustering}
        At this step, \textit{Affinity propagation} is applied on the tSNE result data plane, several damping parameters were tested and presented further at the error metrics section. 
        After the clustering, each cluster center point into the resulting plane receives a human-readable label, as shown on table \ref{tab:labeled_clusters} example, also, chart \ref{fig:belford} shows the distribution of consumed topics by a sampled user resembling a Benford's law distribution:
        
        
        \begin{table}[H]
            \centering
                \begin{tabular}{ |c|c|c| } 
                \hline
                x & y & label \\
                \hline 
                ... & ...  & ... \\
                19.699291 & -28.117729 & tangerine-events \\ 
                -8.711248 & 21.906485 & purple-occurrences \\ 
                11.325381 & 3.893677 & yellow-episodes \\ 
                ... & ...  & ...  \\
                \hline
                \end{tabular}
            \caption{Labeled cluster centers}
            \label{tab:labeled_clusters}
        \end{table}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{images/belford.png}
            \caption{Topic frequencies from a sampled user}
            \label{fig:belford}
        \end{figure}

% =======================================================================

\chapter{Results}

    Here there will be a comparison between \(euclidean\ distance\) threshold against \(alternate\ least\ square\) methods and its parameters variations, different combinations of items and sessions were performed like the impact of ordering or shuffling items by timestamp, different affinity propagation damping factors and different sizes of sessions.
    
    On section \ref{globo_dot_com_dataset} Globo dataset properties are presented, on \ref{euclidean_distance_heuristic}, section \ref{session_simulation} the simulation of sessions as metric method is adressed with its results and on \ref{affinity_propagation_results} heatmaps from iterations are shown.

    % TODO como foi separado test data vs non-test data?
    % TODO talvez seja interessante uma seção de performance(precision and recall) e limites do algoritmo (e do método de clusterização)
    % TODO seção pode ser chamada apenas de Datasets
    \section{Globo.com dataset} \label{globo_dot_com_dataset}
    %TODO contextualização disso nesse capítulo
    It is a collection that consists on the following:
    \begin{itemize}
        \item approximately 2.8 millions news clicks
        \item more than 300 thousand users
        \item 45 thousand news articles
        \item 250 dimensional feature matrix
    \end{itemize}
    
    Since the availability of articles text data were not allowed by the company by commercial reasons, at \cite{deSouzaPereiraMoreira:2018:CDL:3240323.3240331} presented CHAMELEON, a RNN algorithm for feature extraction, and together with the dataset, they allowed the use of an feature embedding file extracted from its novel algorithm. Every news from the dataset have its own 250 feature dimensions on the embedding matrix
    
    The dataset consists on the following data for each click as the columns:
    \newline \textit{user\_id, session\_id, session\_start, session\_size, click\_article\_id, click\_timestamp,}
    \newline \textit{click\_environment, click\_deviceGroup, click\_os, click\_country, click\_region,}
    \newline \textit{click\_referrer\_type} 

    sample showed on the table below
    
    \begin{table}[!ht]
        \centering
        \begin{tabular}{ |c|c|c|c|c|c|c|c|c|c| } 
            \hline
            click\_timestamp & user\_id & article\_id & click\_country & click\_region & ... \\
            \hline 
            ... & ...  & ...  & ...  & ... & \\
            1506826800026 & 59 & 234853 & 1 & 21 & \\ 
            1506826801702 & 79 & 159359 & 1 & 13 & ...\\ 
            1506826804207 & 154 & 96663 & 1 & 25 & \\ 
            ... & ...  & ...  & ...  & ... & \\
            \hline
        \end{tabular}
        \caption{Globo dataset sample as Dataframe}
        \label{tab:globo_dataset_sample}
    \end{table}


    \section{Euclidean Distance as threshold} \label{euclidean_distance_heuristic}
    The chosen method for finding session boundaries was based on the will that users tend to encircle same or similar topics, similarly as shown on chart \ref{fig:belford}, on that sense, this hypothesis was used to create a model using euclidean distance from topics to measure how different they are.
    
    \begin{definition}
    In a session \(S\) with a set of items \(i_{1}, i_{2}, ...\) that each item has a \textit{topic} \(t\) associated, where each \textit{topic} \(t\) has a known coordinate \(x, y\) in a Cartesian plane.
    If the euclidean distance between the topic \(t_{p}\) of item \(i_{n}\) and the topic \(t_{q}\) of item \(i_{n+1}\) is higher than a chosen cutoff \(c\), than it is considered an end of a session and beginning of another.
    \label{euclidean_distance_threshold_definition}
    \end{definition}

    
    The error metrics from cutoff heuristic was used as follow:
    
        
        \begin{itemize}
            \item The cutoff \(c\) is a row index that belongs to one of the sessions, this index is found by the cutoff trigger heuristic at definition \ref{euclidean_distance_threshold_definition}
            \item Because the two sessions doesn't have the same amount of items, thus asymmetric, a normalization mapping of the error metric is performed.
            \item It is calculated the difference between the number of items from the inferred index(found by heuristic) and the perfect index(the real cutoff) and divided by the total number of items at the artificial session (made by two real sessions), as represented above in the gray-colored table by the error column:
            
            \begin{table}[!ht]
                \centering
                \begin{tabular}{c|c|c}
                    \hline
                    \rowcolor[RGB]{239,239,239}
                    index & user\_id & error \\
                    \hline 
                    \rowcolor[RGB]{140,140,140}
                    0 & 111 & 1 \\ 
                    \rowcolor[RGB]{197,197,197}
                    1 & 111 & 0.66 \\ 
                    \rowcolor[RGB]{226,226,226}
                    2 & 111 & 0.33 \\ 
                    \rowcolor[RGB]{255,255,255}
                    3 & 888 & 0 \\ 
                    \rowcolor[RGB]{239,239,239}
                    4 & 888 & 0.16 \\ 
                    \rowcolor[RGB]{222,222,222}
                    5 & 888 & 0.33 \\ 
                    \rowcolor[RGB]{206,206,206}
                    6 & 888 & 0.5 \\ 
                    \rowcolor[RGB]{189,189,189}
                    7 & 888 & 0.66 \\ 
                    \rowcolor[RGB]{173,173,173}
                    8 & 888 & 0.83 \\ 
                    \rowcolor[RGB]{140,140,140}
                    9 & 888 & 1 \\ 
                    \hline
                \end{tabular}
                \caption{Error normalization map from desired cutoff index}
                \label{tab:cutoff_table}
            \end{table}
            
            So in this example, the index 3 is the perfect cutoff, representing the real threshold between the two sessions, the picked index by the cutoff heuristic, will receive the respective error from its index row.
        \end{itemize} 

        % TODO subseção com apenas 1 parágrafo
        \section{Sessions simulation} \label{session_simulation}
        Because the dataset already have labeled the respective \textit{user id} in each row, it was considered that the most part of accounts were in fact from individual users, in that case, it is assumed that even if some users really share the same account, this would be statistically irrelevant when analyzing the whole dataset.
        Considering this scenario true, the simulated sessions is constructed as a concatenation of two streams of two separated user ids 
        as addressed on table \ref{session_simulation} a sample from a simulated session.
        
        Notice that the column \textit{distance} is the euclidean distance from current centroid of row index \(n\) and centroid from row index \(n - 1\)
        
        \begin{table}[!ht]
            \centering
            \begin{tabular}{ |c|c|c|c|c|c| } 
                \hline
                \  & click\_timestamp & user\_id & x\_centroid & y\_centroid & distance \\
                \hline 
                 ... & ... & ... & ... & ... & ... \\
                 11 & 1507987156229 & 6207 & -8.487595 & 3.206146 & 43.517998 \\
                 12 & 1507988999717 & 6207 & 3.864952 & -3.751176 & 27.925744 \\
                 13 & 1507989029717 & 6207 & 5.004131 & 3.442250 & 7.283071 \\
                 \rowcolor[RGB]{220,220,220}
                 14 & 1507061447189 & 143259 & 25.503668 & -1.358489 & 21.054171 \\
                 15 & 1507061615464 & 143259 & 18.530073 & -15.994876 & 16.212799 \\
                 16 & 1507061615464 & 143259 & 18.530073 & -15.994876 & 0.000000 \\
                 ... & ... & ... & ... & ... & ... \\
                \hline
            \end{tabular}
            \caption{Simulated session sample with desired cut in gray}
            \label{tab:session_simulation}
        \end{table}
        
        \section{Temporal ordering factor} \label{temporal_ordering_factor}
        
        Since time ordering can be sensitive for news articles a method to disambiguate the influence of temporal order was introduced because the objective of this work is to segregate sessions exclusively on its content excluding time factor for example.
        
        There were two different types of measurements, one of them had time ordering were each user session had its items ordered by its timestamp, on the second one, the items were completely shuffled, removing time influence.
        
        A floor cutoff distance was pick as 1, and the ceiling as 60, with this limits defined, 15 linearly spaced cutoff distances were generated as input as shown by columns \textit{cutoff dist} on table \ref{tab:cutoff_timestamp_ordered_shuffled}, for each session cut attempt, an error were calculated and attributed as section \ref{euclidean_distance_heuristic} describes, an array of error metrics was created and an \textit{error mean} and also an \textit{error standard deviation}, described on second and third column of table \ref{tab:cutoff_timestamp_ordered_shuffled}.
        
        Looking on both mean and std metrics, it has a neat meaning that time order does not affect the euclidean threshold heuristic, however, it is important to notice the high error mean (more than 0.5), furthermore, the results shows that the lower the error mean, the higher the std by a significant factor rising considerably the \textit{coefficient of variation} (\(std \div mean\) ), thus making it less reliable. For example, on table \ref{tab:cutoff_timestamp_ordered_shuffled}, notice that on the table with time ordering, the lowest error mean is 0.611 and its standard deviation mean is 0.3 or in other words, a deviation of 30 \%, making this cutoff parameter unreliable for the session cutting purpose.
        
        \begin{table}[ht!]
        \centering
        \subfloat{
            \begin{tabular}{ |c|c|c| } 
                \hline
                cutoff dist. & error mean & error std \\
                \hline 
                1.00 & 0.933 & 0.053 \\
                5.21 & 0.932 & 0.054 \\
                9.42 & 0.922 & 0.071 \\
                13.64 & 0.917 & 0.078 \\
                17.85 & 0.901 & 0.098 \\
                22.07 & 0.877 & 0.126 \\
                26.28 & 0.857 & 0.151 \\
                30.5 & 0.812 & 0.191 \\
                34.71 & 0.765 & 0.230 \\
                38.92 & 0.681 & 0.266 \\
                43.14 & 0.629 & 0.286 \\
                43.35 & 0.611 & 0.301 \\
                51.57 & 0.630 & 0.311 \\
                55.78 & 0.707 & 0.320 \\
                60.00 & 0.814 & 0.287 \\
                \hline
            \end{tabular}
        }
        \rule{1cm}{0cm}
        \subfloat{
            \begin{tabular}{ |c|c|c| } 
                \hline
                cutoff dist. & error mean & error std \\
                \hline 
                1.00 & 0.939 & 0.045 \\
                5.21 & 0.938 & 0.048 \\
                9.42 & 0.933 & 0.057 \\
                13.64 & 0.927 & 0.663 \\
                17.85 & 0.912 & 0.088 \\
                22.07 & 0.893 & 0.114 \\
                26.28 & 0.867 & 0.144 \\
                30.5 & 0.827 & 0.184 \\
                34.71 & 0.778 & 0.223 \\
                38.92 & 0.710 & 0.264 \\
                43.14 & 0.646 & 0.289 \\
                43.35 & 0.615 & 0.310 \\
                51.57 & 0.631 & 0.322 \\
                55.78 & 0.721 & 0.325 \\
                60.00 & 0.813 & 0.298 \\
                \hline
            \end{tabular}
        }
        \caption{Error metrics, session ordered by timestamp(left) vs shuffled(right)}
        \label{tab:cutoff_timestamp_ordered_shuffled}
        \end{table}
    
    \newpage
    \section{Affinity propagation damping factor} \label{affinity_propagation_results}
        Another possibility for interference on session cuts performance can be the granularity of clustering, or in other words, how specific or generic that a topic classification should be so that it is enough to recognize the difference between users sharing an account? 
        
        In a single topic like Math, many areas can be classified like algebra, calculus, statistic and so on, at the same time, you can be more generic, and simply address that all those areas are simply Math, and then you segregate other topics as Portuguese, Biology, Math and so on without opening into details.
        
        Since the \textit{damping factor} from the Affinity Propagation Algorithm describe on \ref{algorithms} changes the amount of classified clusters, another parameter was introduced on the error metrics calculations and that being the \texit{damping factor}. For different damping factors, the amount of topics is addressed on table \ref{tab:damping_factor}
        
        \begin{table}[H]
            \centering
                \begin{tabular}{ |c|c| } 
                \hline
                damping & nº of clusters \\
                \hline 
                0.5 & 4771 \\ 
                0.553 & 4716 \\ 
                0.606 & 2984 \\
                0.66 & 1146 \\
                0.766 & 91 \\
                0.82 & 152 \\
                \hline
                \end{tabular}
            \caption{damping factor effect on nº of clusters}
            \label{tab:damping_factor}
        \end{table}
        
        The damping parameter was fixed at 0.9 and similarly as \ref{temporal_ordering_factor} a number of 15 cutoff values were tested between values 1 and 60, linearly spaced for each iteration with a different cutoff . The output was a vector of error values of the same size of the simulated sessions(one for each session, about 32 thousand sessions).
        Then, from this error vector, it was calculated its mean and standard deviation for each iteration (15 iterations), so that we could measure its reliability and precision.
        
        

    
        The heatmaps error results are interpreted as the following:
        \begin{itemize}
            \item The Vertical axis of the represents the affinity propagation damping factor
            \item The Horizontal axis corresponds to the chosen cutoff distance threshold
            \item the value of each item on the heatmap corresponds to one of two metric errors: Mean error, Standard deviation error.
        \end{itemize}
    
        The objective of this metric is find a lower than 50\% mean error and the lowest \textbf{coefficient of variation} (\(std \div mean\) ). 
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.95\textwidth]{images/error_mean.png}
            \caption{Error mean heatmap}
            \label{fig:error_mean_heatmap}
        \end{figure}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.98\textwidth]{images/error_std.png}
            \caption{Error standard deviation heatmap}
            \label{fig:error_std_heatmap}
        \end{figure}
    
    \section{Applying alternate least squares}
        \subsection{Method}
        In order to avoid cold start, only users with more than 200 items in total were chosen.
        Like in the Euclidean Distance experiment, fictitious sessions were created, but instead of distance column as error metric, a new column 'prediction' were added that corresponds to the predicted probability that one user have to read that article, the chosen user is the user of the first session.
        

        The expected result is that, on the first session, the prediction should have low standard deviation and a high mean, and on the second session, using the firsts session user\_id, as prediction parameter, the expected mean, in case of different user styles, should be lower and also the standard deviation.
        
        \newpage 
        
        % TODO subseção com apenas 1 parágrafo
        \subsection{Alternate Least Squares Error Metrics}
        The following table was created as result, it was extracted metrics from 413 users.
        \begin{table}[H]
            \centering
            \begin{tabular}{ |c|c|c|c| } 
                \hline
                user\_id & user\_id\_to\_predict & prediction\_mean & prediction\_std \\
                \hline 
                413 & 413 & 0.83287 & 0.2799 \\
                45502 & 413 & 0.86549 & 0.25900 \\
                \rowcolor[RGB]{220,220,220}
                12897 & 12897 & 0.84201 & 0.3291 \\
                \rowcolor[RGB]{220,220,220}        
                16695 & 12897 & 0.91515 & 0.27717 \\
                2930 & 2930 & 0.79370 & 0.34552 \\
                9261 & 2930 & 0.87126 & 0.2551 \\
                \rowcolor[RGB]{220,220,220}        
                20001 & 20001 & 0.8777 & 0.24580 \\
                \rowcolor[RGB]{220,220,220}        
                6344 & 20001 & 0.91090 & 0.2383 \\
                62025 & 62025 & 0.75506 & 0.35445 \\
                19864 & 62025 & 0.47764 & 0.27196 \\
                \rowcolor[RGB]{220,220,220}        
                43017 & 43017 & 0.83342 & 0.27467 \\
                \rowcolor[RGB]{220,220,220}        
                21356 & 43017 & 0.81031 & 0.28030 \\
                3391 & 3391 & 0.85998 & 0.2949 \\
                681 & 3391 & 0.87366 & 0.30739 \\
                \rowcolor[RGB]{220,220,220}        
                11521 & 11521 & 0.61621 & 0.3742 \\
                \rowcolor[RGB]{220,220,220}                
                59193 & 11521 & 1.01384 & 0.5024 \\
                11359 & 11359 & 0.88445 & 0.2596 \\
                23036 & 11359 & 0.91379 & 0.24409 \\
                \rowcolor[RGB]{220,220,220}                
                22301 & 22301 & 0.81135 & 0.30362 \\
                \rowcolor[RGB]{220,220,220}                
                77985 & 22301 & 0.78393 & 0.32523 \\
                13885 & 13885 & 0.85638 & 0.264 \\
                9193 & 13885 & 0.80591 & 0.2890 \\
                \hline
            \end{tabular}
            \caption{Alternate Least Squares measurements}
            \label{tab:alternate_least_squares_metrics}
        \end{table}

% =======================================================================

\chapter{Conclusion}
The standard deviation presented on the heatmaps with different input parameters reveals that the euclidean distance as threshold method does not have enough precision, but these results could have many explanations for this lack of precision not only from the method itself. Since globo dataset is closed source and the features available from \cite{deSouzaPereiraMoreira:2018:CDL:3240323.3240331} were created from his own algorithm, a certain level of belief was implied that the features really reflected its content and his algorithm worked well on quantitatively describing them.

Another possible cause could be the dimensionality reduction step, no other algorithm was tested but the recommend tSNE from \cite{deSouzaPereiraMoreira:2018:CDL:3240323.3240331}.

Finally, news as a type of media have a chaotic nature, even when readers have his preference for certain topics, unexpected and relevant news from other topics may arrive and catch the readers eyes those phenomena, maybe they are common and can hinder this method of session cut by euclidean distance threshold. 

\bibliographystyle{packages/abntex2-alf}
\bibliography{biblio}

\end{document}
